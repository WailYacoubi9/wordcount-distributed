# ðŸ§ª Guide de Test - Mono-Site et Multi-Site

## ðŸ“‹ PrÃ©requis

Avant de commencer, assure-toi que:
- âœ… Tu as accÃ¨s Ã  Grid5000
- âœ… Tu as un compte Grid5000 configurÃ©
- âœ… Tu peux te connecter via SSH

---

## ðŸ”§ PrÃ©paration Initiale

### 1. Connexion Ã  Grid5000

```bash
# Depuis ton ordinateur local
ssh <ton-login>@access.grid5000.fr

# Exemple
ssh wailyacoubi@access.grid5000.fr
```

### 2. Choisir un site de dÃ©part

```bash
# Connexion Ã  un site (choisis-en un)
ssh grenoble    # ou nancy, lyon, rennes, etc.
```

### 3. Cloner/Mettre Ã  jour le projet

```bash
# Si pas encore clonÃ©
git clone https://github.com/WailYacoubi9/wordcount-distributed.git
cd wordcount-distributed

# Si dÃ©jÃ  clonÃ©
cd wordcount-distributed
git checkout claude/test-repo-grid-support-01SFCU975DhRRWrpAoX8krpW
git pull origin claude/test-repo-grid-support-01SFCU975DhRRWrpAoX8krpW
```

### 4. Compiler le projet

```bash
# Compiler Java
javac -d bin -sourcepath src $(find src -name '*.java')

# Compiler C
gcc -o wordcount test/wordcount.c

# VÃ©rifier
ls -lh bin/scheduler/Main.class
ls -lh wordcount
```

---

## ðŸŽ¯ TEST 1: MONO-SITE (Version SCP)

### Ã‰tape 1: RÃ©server des nÅ“uds sur UN SEUL site

```bash
# RÃ©servation interactive (recommandÃ© pour tests)
oarsub -I -l nodes=4,walltime=1:00:00

# Tu verras quelque chose comme:
# [ADMISSION RULE] Modify resource description with type constraints
# OAR_JOB_ID=123456
# Interactive mode: waiting...
# Starting...

# VÃ©rifier les nÅ“uds rÃ©servÃ©s
cat $OAR_NODEFILE
# dahu-1.grenoble.grid5000.fr
# dahu-1.grenoble.grid5000.fr
# dahu-1.grenoble.grid5000.fr
# dahu-1.grenoble.grid5000.fr
# dahu-2.grenoble.grid5000.fr
# ...
```

### Ã‰tape 2: Lancer le test mono-site

```bash
cd ~/wordcount-distributed

# Option A: Script mono-site classique
./deploy/run_mono_site.sh

# Option B: Script universel (dÃ©tecte auto)
./deploy/run_universal.sh
```

### Ã‰tape 3: Observer l'exÃ©cution

Tu verras:
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   MONO-SITE DISTRIBUTED WORD COUNT                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ“ Site: grenoble
ðŸ–¥ï¸  Master node: dahu-1.grenoble.grid5000.fr

ðŸ‘· Worker nodes:
  - dahu-2.grenoble.grid5000.fr
  - dahu-3.grenoble.grid5000.fr
  - dahu-4.grenoble.grid5000.fr

âœ… All nodes confirmed on site: grenoble

ðŸ“¦ Copying files to worker nodes...
  - Copying to dahu-2.grenoble.grid5000.fr...
  - Copying to dahu-3.grenoble.grid5000.fr...
  - Copying to dahu-4.grenoble.grid5000.fr...
âœ… Files copied successfully

ðŸš€ Starting worker nodes...
â³ Waiting for workers to initialize...

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   STARTING DISTRIBUTED EXECUTION                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[SCHEDULER] Starting task execution...
[TASK count1.txt] âœ… Completed successfully
[TASK count2.txt] âœ… Completed successfully
[TASK count3.txt] âœ… Completed successfully
[TASK count4.txt] âœ… Completed successfully
[TASK count5.txt] âœ… Completed successfully
[TASK total.txt] ðŸ“Š Running aggregation locally on master node
[TASK total.txt] âœ… Local execution successful

âœ… Execution completed successfully!

ðŸ“Š RESULTS:
  Total word count: 75000

  Individual counts:
    - part1.txt: 15000 words
    - part2.txt: 15000 words
    - part3.txt: 15000 words
    - part4.txt: 15000 words
    - part5.txt: 15000 words
```

### Ã‰tape 4: VÃ©rifier les rÃ©sultats

```bash
# Voir le total
cat total.txt
# 75000

# Voir les dÃ©tails
cat count*.txt
# 15000
# 15000
# 15000
# 15000
# 15000

# VÃ©rifier les logs des workers
ls worker.log
```

### Ã‰tape 5: LibÃ©rer les ressources

```bash
# Le script fait le cleanup automatiquement, mais tu peux vÃ©rifier:
exit  # Sortir de la rÃ©servation OAR
```

---

## ðŸŒ TEST 2: MULTI-SITE (Version SCP)

### MÃ©thode A: Avec oargridsub (RecommandÃ©)

```bash
# Sur access.grid5000.fr
ssh access.grid5000.fr

# RÃ©server sur plusieurs sites en une commande
oargridsub -w 1:00:00 \
  grenoble:rdef="/nodes=2" \
  lyon:rdef="/nodes=2"

# Tu recevras un grid job ID
# Grid job ID: 12345

# Se connecter au site master (premier site)
ssh grenoble

# Ton OAR_NODEFILE contient dÃ©jÃ  les nÅ“uds des deux sites
cat $OAR_NODEFILE
# dahu-1.grenoble.grid5000.fr
# dahu-2.grenoble.grid5000.fr
# nova-1.lyon.grid5000.fr
# nova-2.lyon.grid5000.fr

# Lancer le test
cd ~/wordcount-distributed
./deploy/run_multi_site.sh

# OU avec le script universel
./deploy/run_universal.sh
```

### MÃ©thode B: RÃ©servation manuelle (Plus de contrÃ´le)

```bash
# Terminal 1: Site 1 (Grenoble)
ssh access.grid5000.fr
ssh grenoble
oarsub -I -l nodes=2,walltime=1:00:00

# Sauvegarder les nÅ“uds de Grenoble
cat $OAR_NODEFILE > ~/nodefile_grenoble
uniq $OAR_NODEFILE > ~/combined_nodefile

# Garder ce terminal ouvert!
```

```bash
# Terminal 2: Site 2 (Lyon)
ssh access.grid5000.fr
ssh lyon
oarsub -I -l nodes=2,walltime=1:00:00

# Sauvegarder les nÅ“uds de Lyon
cat $OAR_NODEFILE > ~/nodefile_lyon
uniq $OAR_NODEFILE > ~/combined_nodefile_lyon
```

```bash
# Retour au Terminal 1 (Grenoble - Master)

# RÃ©cupÃ©rer les nÅ“uds de Lyon
MASTER=$(hostname)
scp lyon:~/combined_nodefile_lyon ~/

# Combiner les nodefiles
cat ~/combined_nodefile_lyon >> ~/combined_nodefile

# VÃ©rifier le nodefile combinÃ©
cat ~/combined_nodefile
# dahu-1.grenoble.grid5000.fr
# dahu-2.grenoble.grid5000.fr
# nova-1.lyon.grid5000.fr
# nova-2.lyon.grid5000.fr

# Exporter le nodefile combinÃ©
export OAR_NODEFILE=~/combined_nodefile

# Lancer le test multi-site
cd ~/wordcount-distributed
./deploy/run_multi_site.sh
```

### Observer l'exÃ©cution multi-site

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   MULTI-SITE DISTRIBUTED WORD COUNT                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ“ Master site: grenoble
ðŸ–¥ï¸  Master node: dahu-1.grenoble.grid5000.fr

ðŸ—ºï¸  Analyzing site distribution...

Sites involved:
  âœ“ grenoble: 2 node(s) [MASTER SITE]
  â†’ lyon: 2 node(s)

âœ… Multi-site deployment confirmed (2 sites)

ðŸ‘· Worker nodes by site:
  [grenoble] dahu-2.grenoble.grid5000.fr
  [lyon] nova-1.lyon.grid5000.fr
  [lyon] nova-2.lyon.grid5000.fr

Total workers: 3 across 2 sites

ðŸ“¡ Multi-site network information:
  - Nodes use fully qualified domain names (FQDN)
  - RMI communication may require specific network configuration
  - Latency between sites: typically 1-10ms depending on sites

ðŸ“¦ Copying files to worker nodes (this may take longer for remote sites)...
  - [grenoble] Copying to dahu-2.grenoble.grid5000.fr...
  - [lyon] Copying to nova-1.lyon.grid5000.fr...
  - [lyon] Copying to nova-2.lyon.grid5000.fr...
âœ… Files copied to all sites

ðŸš€ Starting worker nodes across all sites...
â³ Waiting for workers to initialize across all sites... (8 seconds)

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   STARTING MULTI-SITE DISTRIBUTED EXECUTION             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[SCHEDULER] Starting task execution...
[TASK count1.txt] Assigned to: dahu-2.grenoble.grid5000.fr âœ…
[TASK count2.txt] Assigned to: nova-1.lyon.grid5000.fr âœ…
[TASK count3.txt] Assigned to: nova-2.lyon.grid5000.fr âœ…
[TASK total.txt] ðŸ“Š Running aggregation locally on master node
[TASK total.txt] âœ… Local execution successful

âœ… Multi-site execution completed successfully!
Execution time: 12s

ðŸ“Š RESULTS:
  Total word count: 75000

ðŸŒ Multi-site performance:
  Sites involved: 2
  Workers: 3
  Execution time: 12s
```

---

## ðŸŽ¯ TEST 3: TEST AVEC FICHIER UTILISATEUR

### Test Mono-Site avec Fichier Utilisateur

```bash
# RÃ©server des nÅ“uds
oarsub -I -l nodes=4,walltime=1:00:00

cd ~/wordcount-distributed

# CrÃ©er ton fichier de test
cat > mydata.txt << 'EOF'
Ceci est mon fichier de test personnel.
Grid5000 est une infrastructure de recherche.
SystÃ¨me distribuÃ© de comptage de mots.
Java RMI pour la communication.
Makefile pour les dÃ©pendances.
EOF

# Lancer avec le script fichier utilisateur
./deploy/run_user_file.sh mydata.txt

# Observer
# Le script va:
# 1. DÃ©tecter 3 workers
# 2. Splitter mydata.txt en 3 parts
# 3. GÃ©nÃ©rer Makefile automatiquement
# 4. ExÃ©cuter le comptage
# 5. Afficher le rÃ©sultat
```

### Test Multi-Site avec Fichier Utilisateur

```bash
# AprÃ¨s avoir rÃ©servÃ© sur 2+ sites et combinÃ© les nodefiles
export OAR_NODEFILE=~/combined_nodefile

cd ~/wordcount-distributed

# CrÃ©er un fichier plus volumineux
cat > large_test.txt << 'EOF'
[Ton texte ici - peut Ãªtre trÃ¨s long]
EOF

# Lancer
./deploy/run_user_file.sh large_test.txt

# Le systÃ¨me s'adaptera automatiquement au nombre de workers
```

---

## ðŸ§ª TEST 4: VERSION NFS (Optionnel)

### Mono-Site NFS

```bash
oarsub -I -l nodes=4,walltime=1:00:00

cd ~/wordcount-distributed

# CrÃ©er fichier test
echo "Test NFS mono-site" > test_nfs.txt

# Lancer
./deploy/run_nfs_mono_site.sh test_nfs.txt
```

### Multi-Site NFS

```bash
# AprÃ¨s combinaison des nodefiles
export OAR_NODEFILE=~/combined_nodefile

./deploy/run_nfs_multi_site.sh ~/combined_nodefile test_nfs.txt
```

---

## âœ… VÃ©rifications Importantes

### 1. VÃ©rifier que les workers ont dÃ©marrÃ©

```bash
# Sur le master, pendant l'exÃ©cution
for host in $(uniq $OAR_NODEFILE | tail -n +2); do
    echo "=== Worker: $host ==="
    ssh $host "ps aux | grep WorkerNode | grep -v grep"
done
```

### 2. VÃ©rifier les logs des workers

```bash
# Sur un worker
ssh dahu-2 "cat ~/worker.log"
```

### 3. VÃ©rifier la communication RMI

```bash
# Sur le master
netstat -an | grep 3000  # Port RMI par dÃ©faut
```

### 4. VÃ©rifier les fichiers transfÃ©rÃ©s

```bash
# Sur un worker
ssh dahu-2 "ls -lh ~/"
# Devrait voir: bin/, wordcount, part*.txt, Makefile
```

---

## ðŸ” Troubleshooting

### ProblÃ¨me: "OAR_NODEFILE not found"
```bash
# Solution: VÃ©rifier que tu es dans une rÃ©servation OAR
echo $OAR_NODEFILE
# Si vide, tu n'es pas dans une rÃ©servation
oarsub -I -l nodes=4,walltime=1:00:00
```

### ProblÃ¨me: "Failed to copy files to worker"
```bash
# VÃ©rifier la connectivitÃ© SSH
ssh dahu-2 "hostname"

# VÃ©rifier les clÃ©s SSH
ssh-copy-id dahu-2
```

### ProblÃ¨me: "All workers busy, waiting..."
```bash
# Les workers ne rÃ©pondent pas
# VÃ©rifier qu'ils tournent:
for host in $(uniq $OAR_NODEFILE | tail -n +2); do
    ssh $host "ps aux | grep WorkerNode"
done

# Si aucun worker, les redÃ©marrer manuellement
```

### ProblÃ¨me: RÃ©sultat incorrect (ex: 43041 au lieu de 75000)
```bash
# VÃ©rifier que c'est bien la version avec le fix d'agrÃ©gation
grep "Running aggregation locally" total.txt

# VÃ©rifier que tous les count*.txt existent
ls -lh count*.txt

# VÃ©rifier le contenu
cat count*.txt
```

---

## ðŸ“Š RÃ©sultats Attendus

### Mono-Site (5 workers):
```
Total: 75000 mots
count1.txt: 15000
count2.txt: 15000
count3.txt: 15000
count4.txt: 15000
count5.txt: 15000
```

### Multi-Site (Grenoble + Lyon):
```
Total: 75000 mots
Distribution des tÃ¢ches entre les deux sites
Temps d'exÃ©cution: lÃ©gÃ¨rement plus long (quelques secondes)
```

### Fichier Utilisateur:
```
Total: dÃ©pend de ton fichier
Nombre de parts = nombre de workers
Division Ã©quitable (Â±1 ligne)
```

---

## ðŸŽ¯ Checklist de Test ComplÃ¨te

### Phase 1: PrÃ©paration
- [ ] Connexion Ã  Grid5000
- [ ] Projet clonÃ© et Ã  jour
- [ ] Code compilÃ© (Java + C)

### Phase 2: Mono-Site
- [ ] RÃ©servation rÃ©ussie (oarsub)
- [ ] Script lancÃ© (run_mono_site.sh)
- [ ] Workers dÃ©marrÃ©s
- [ ] ExÃ©cution complÃ¨te
- [ ] RÃ©sultat correct (75000)

### Phase 3: Multi-Site
- [ ] RÃ©servation sur 2+ sites
- [ ] Nodefiles combinÃ©s
- [ ] Script lancÃ© (run_multi_site.sh)
- [ ] Workers sur plusieurs sites
- [ ] Communication inter-sites OK
- [ ] RÃ©sultat correct (75000)

### Phase 4: Fichier Utilisateur
- [ ] Fichier crÃ©Ã©
- [ ] run_user_file.sh lancÃ©
- [ ] Makefile gÃ©nÃ©rÃ© automatiquement
- [ ] Split Ã©quitable
- [ ] RÃ©sultat correct

### Phase 5: Version NFS (Optionnel)
- [ ] NFS configurÃ©
- [ ] run_nfs_mono_site.sh testÃ©
- [ ] run_nfs_multi_site.sh testÃ©
- [ ] RÃ©sultats identiques Ã  SCP

---

## ðŸš€ Commandes Rapides (RÃ©sumÃ©)

```bash
# MONO-SITE
ssh grenoble.grid5000.fr
oarsub -I -l nodes=4,walltime=1:00:00
cd ~/wordcount-distributed
./deploy/run_mono_site.sh

# MULTI-SITE
oargridsub -w 1:00:00 grenoble:rdef="/nodes=2" lyon:rdef="/nodes=2"
ssh grenoble
cd ~/wordcount-distributed
./deploy/run_multi_site.sh

# FICHIER UTILISATEUR
echo "Mon texte" > myfile.txt
./deploy/run_user_file.sh myfile.txt

# UNIVERSEL (dÃ©tecte auto)
./deploy/run_universal.sh
```

---

## ðŸ“š Documentation SupplÃ©mentaire

- Grid5000: https://www.grid5000.fr/
- OAR: https://oar.imag.fr/
- README.md du projet
- NFS_USAGE.md (pour version NFS)
- UNIVERSAL_SCRIPT_EXPLAINED.md (pour script universel)

Bon tests! ðŸŽ‰
